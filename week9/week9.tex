\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{hyperref}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}% expected value

\graphicspath{{./images/}}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClassShort\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem {#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
    % \stepcounter{#1}
    \nobreak\extramarks{{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}

\newcommand{\problemNumber}{0.0}

\newenvironment{homeworkProblem}[1][-1]{
    \renewcommand{\problemNumber}{{#1}}
    \section{\problemNumber}
    \setcounter{partCounter}{1}
    \enterProblemHeader{\problemNumber}
}{
    \exitProblemHeader{\problemNumber}
}

%
% Homework Details
%   - Title
%   - Class
%   - Author
%

\newcommand{\hmwkTitle}{Chapter\ \#7 Assignment}
\newcommand{\hmwkClassShort}{RBE 595}
\newcommand{\hmwkClass}{RBE 595 --- Reinforcement Learning}
\newcommand{\hmwkAuthorName}{\textbf{Arjan Gupta}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass}}\\
    \textmd{\textbf{\hmwkTitle}}\\
    \textmd{\textbf{n-step Bootstrapping}}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d}}{\mathrm{d}#2} \left(#1\right)}

% For compact derivatives
\newcommand{\derivcomp}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #2} \left(#1\right)}

% For compact partial derivatives
\newcommand{\pderivcomp}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\nobreak\extramarks{Problem 1}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 1]
    The first episode of an agent interacting with an environment under policy $\pi$ is as follows:

    \begin{center}
        \begin{tabular}{c c c c}
            Timestep & Reward & State & Action\\
            \hline
            0 &  & X & U1\\
            1 & 16 & X & U2\\
            2 & 12 & X & U1\\
            3 & 24 & X & U1\\
            4 & 16 & T & \\
        \end{tabular}
    \end{center}

    Assume discount factor, $\gamma = 0.5$, step size $\alpha = 0.1$ and $q_{\pi}$ is initially zero.
    What are the estimates of $q_{\pi}(X, U1)$ and $q_{\pi}(X, U2)$ using 2-step SARSA\@?

    \subsection{Answer}

    The estimates of $q_{\pi}(X, U1)$ and $q_{\pi}(X, U2)$ using 2-step SARSA are as follows:

    \subsubsection{Timestep 0}
    \begin{align*}
        q_{\pi}(X, U1) &= q_{\pi}(X, U1) + \alpha \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}) - q_{\pi}(X, U1) \right]\\
                      &= 0 + 0.1 \left[ 16 + 0.5 \cdot 12 + 0.5^2 \cdot 0 - 0 \right]\\
                      &= 0 + 0.1 \left[ 16 + 6 - 0 \right]\\
                      &= 0 + 0.1 \left[ 22 \right]\\
                      &= 0 + 2.2\\
                      &= 2.2
    \end{align*}

    \subsubsection{Timestep 1}
    \begin{align*}
        q_{\pi}(X, U2) &= q_{\pi}(X, U2) + \alpha \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}) - q_{\pi}(X, U2) \right]\\
                      &= 0 + 0.1 \left[ 12 + 0.5 \cdot 24 + 0.5^2 \cdot q_{\pi}(X, U1) - 0 \right]\\
                      &= 0 + 0.1 \left[ 12 + 12 + 0.25*2.2 \right]\\
                        &= 0 + 0.1 \left[ 24 + 0.55 \right]\\
                        &= 0 + 0.1 \left[ 24.55 \right]\\
                        &= 2.455
    \end{align*}
    \subsubsection{Timestep 2}
    \begin{align*}
        q_{\pi}(X, U1) &= q_{\pi}(X, U1) + \alpha \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}) - q_{\pi}(X, U1) \right]\\
                      &= 2.2 + 0.1 \left[ 24 + 0.5 \cdot 16 + 0.5^2 \cdot q_{\pi}(T) - 2.2 \right]\\
                      &= 2.2 + 0.1 \left[ 24 + 8 + 0 - 2.2 \right]\\
                      &= 2.2 + 0.1 \left[ 29.8 \right]\\
                        &= 2.2 + 2.98\\
                        &= 5.18
    \end{align*}
    \subsubsection{Timestep 3}

    \begin{align*}
        q_{\pi}(X, U1) &= q_{\pi}(X, U1) + \alpha \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) - q_{\pi}(X, U1) \right]\\
                        &= 5.18 + 0.1 \left[ 16 + 0.5 \cdot q_{\pi}(T) - 5.18 \right]\\
                        &= 5.18 + 0.1 \left[ 16 + 0 - 5.18 \right]\\
                        &= 5.18 + 0.1 \left[ 10.82 \right]\\
                        &= 6.262
    \end{align*}

    Therefore, the estimates of $q_{\pi}(X, U1)$ and $q_{\pi}(X, U2)$ using 2-step SARSA are $6.262$ and $2.455$ respectively.

\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 2]
    What is the purpose of introducing Control Variates in per-decision importance sampling?

    \subsection{Answer}

    The purpose of introducing Control Variates in per-decision importance sampling is
    to further reduce the variance of the estimate of the return.\\
    
    Plain per-decision importance
    sampling reduces the variance of the estimate of the return by making sure that the estimate
    of the whole return is not 0 every time the behavior policy takes an action
    that the target policy would not have taken. However, the variance of the estimate of the
    return can be decreased even further by using Control Variates. This is done
    by using a `control variate term' in the equation for $G_{t:h}$. Without
    control variates, the equation for $G_{t:h}$ is as follows:

    \begin{align*}
        G_{t:h} = R_{t+1} + \gamma G_{t+1:h}, \quad t < h < T
    \end{align*}

    With control variates, the equation for $G_{t:h}$ is as follows:

    \begin{align*}
        G_{t:h} \doteq \rho_{t} \left[ R_{t+1} + \gamma G_{t+1:h}] + (1 - \rho_{t}) V_{h-1}(S_t) \right], \quad t < h < T
    \end{align*}

    Where the control variate term is $(1 - \rho_{t}) V_{h-1}(S_t)$.\\

    This equation uses a `convex combination' of the estimate of the two terms. This way, 
    when $\rho_{t} = 0$, we have $G_{t:h} = V_{h-1}(S_t)$, which is the current
    estimate of the return for the current state. Overall this reduces the `jaggedness' in the
    plot of the estimate of the return over time, which reduces the variance of the estimate
    of the return.\\

    It is also possible to prove that the control variate term does not add bias
    to the estimate of the return.

    \subsubsection{Control variates for action-value estimation}

    With control-estimates, the equation for $G_{t:h}$ in action-value estimation is as follows:

    \begin{align*}
        G_{t:h} = R_{t+1} + \gamma \rho_{t+1} (G_{t+1:h} - Q(S_{t+1}, A_{t+1})) + \gamma V_{h-1}(S_{t+1})
    \end{align*}

    Where $V(S) = \sum_{a} \pi(a|S) Q(S, a)$.\\

    Here, when $\rho_{t} = 0$, we have $G_{t:h} = R_{t+1} + \gamma V_{h-1}(S_{t+1})$, instead
    of simply $G_{t:h} = R_{t+1}$, which would be the case without control variates. This also
    reduces the variance of the estimate of the return.\\

\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\nobreak\extramarks{Problem 3}{}\nobreak{}

\begin{homeworkProblem}[Problem 3]
    In off-policy learning, what are the pros and cons of the Tree-Backup algorithm versus off-policy
    SARSA (comment on the complexity, exploration, variance, and bias, and others)?

    \subsection{Answer}

    The pros and cons of the Tree-Backup algorithm versus off-policy SARSA are as follows:

    \begin{itemize}
        \item \textbf{Complexity:} The complexity of the Tree-Backup algorithm is $O(n)$, where $n$ is the number of steps. The complexity of off-policy SARSA is $O(1)$.
        \item \textbf{Exploration:} The Tree-Backup algorithm explores the environment by following the policy $\pi$ and then following the behavior policy $\mu$ for the remaining steps. Off-policy SARSA explores the environment by following the behavior policy $\mu$ for all steps.
        \item \textbf{Variance:} The variance of the Tree-Backup algorithm is lower than that of off-policy SARSA\@. This is because the Tree-Backup algorithm uses a control variate term to reduce the variance of the estimate.
        \item \textbf{Bias:} The bias of the Tree-Backup algorithm is higher than that of off-policy SARSA\@. This is because the Tree-Backup algorithm uses a control variate term to reduce the variance of the estimate.
        \item \textbf{Others:} The Tree-Backup algorithm is an on-policy algorithm, while off-policy SARSA is an off-policy algorithm.
    \end{itemize}
\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 4}{}\nobreak{}

\begin{homeworkProblem}[Problem 4]
    \textbf{(Exercise 7.4)} Prove that the $n$-step return of Sarsa (7.4) can be written 
    exactly in terms of a novel TD error, as

    \begin{align*}
        G_{t:t+n} = Q_{t-1}(S_t, A_t) + \sum_{k=t}^{min(t+n, T)-1} \gamma^{k-t} [R_{k+1} + \gamma Q_k(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k)]
    \end{align*}

    \subsection{Answer}

    The $n$-step return of Sarsa (7.4) is as follows:

    \begin{align*}
        G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}), \quad n \geq 1, \quad  0 \leq t < T-n
    \end{align*}

    We can rewrite this as follows:

    \begin{align*}
        G_{t:t+n} &= R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n})\\
                  &= R_{t+1} + \gamma R_{t+2} + \ldots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}) - \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}) + \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n})
    \end{align*}

\end{homeworkProblem}

\end{document}