\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{hyperref}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}% expected value

\graphicspath{{./images/}}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClassShort\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem {#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
    % \stepcounter{#1}
    \nobreak\extramarks{{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}

\newcommand{\problemNumber}{0.0}

\newenvironment{homeworkProblem}[1][-1]{
    \renewcommand{\problemNumber}{{#1}}
    \section{\problemNumber}
    \setcounter{partCounter}{1}
    \enterProblemHeader{\problemNumber}
}{
    \exitProblemHeader{\problemNumber}
}

%
% Homework Details
%   - Title
%   - Class
%   - Author
%

\newcommand{\hmwkTitle}{Chapter\ \#7 Assignment}
\newcommand{\hmwkClassShort}{RBE 595}
\newcommand{\hmwkClass}{RBE 595 --- Reinforcement Learning}
\newcommand{\hmwkAuthorName}{\textbf{Arjan Gupta}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass}}\\
    \textmd{\textbf{\hmwkTitle}}\\
    \textmd{\textbf{n-step Bootstrapping}}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d}}{\mathrm{d}#2} \left(#1\right)}

% For compact derivatives
\newcommand{\derivcomp}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #2} \left(#1\right)}

% For compact partial derivatives
\newcommand{\pderivcomp}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\nobreak\extramarks{Problem 1}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 1]
    The first episode of an agent interacting with an environment under policy $\pi$ is as follows:

    \begin{center}
        \begin{tabular}{c c c c}
            Timestep & Reward & State & Action\\
            \hline
            0 &  & X & U1\\
            1 & 16 & X & U2\\
            2 & 12 & X & U1\\
            3 & 24 & X & U1\\
            4 & 16 & T & \\
        \end{tabular}
    \end{center}

    Assume discount factor, $\gamma = 0.5$, step size $\alpha = 0.1$ and $q_{\pi}$ is initially zero.
    What are the estimates of $q_{\pi}(X, U1)$ and $q_{\pi}(X, U2)$ using 2-step SARSA\@?

    \subsection{Answer}

    The estimates of $q_{\pi}(X, U1)$ and $q_{\pi}(X, U2)$ using 2-step SARSA are as follows:

    \subsubsection{Timestep 0}
    \begin{align*}
        q_{\pi}(X, U1) &= q_{\pi}(X, U1) + \alpha \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}) - q_{\pi}(X, U1) \right]\\
                      &= 0 + 0.1 \left[ 16 + 0.5 \cdot 12 + 0.5^2 \cdot 0 - 0 \right]\\
                      &= 0 + 0.1 \left[ 16 + 6 - 0 \right]\\
                      &= 0 + 0.1 \left[ 22 \right]\\
                      &= 0 + 2.2\\
                      &= 2.2
    \end{align*}

    \subsubsection{Timestep 1}
    \begin{align*}
        q_{\pi}(X, U2) &= q_{\pi}(X, U2) + \alpha \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}) - q_{\pi}(X, U2) \right]\\
                      &= 0 + 0.1 \left[ 12 + 0.5 \cdot 24 + 0.5^2 \cdot q_{\pi}(X, U1) - 0 \right]\\
                      &= 0 + 0.1 \left[ 12 + 12 + 0.25*2.2 \right]\\
                        &= 0 + 0.1 \left[ 24 + 0.55 \right]\\
                        &= 0 + 0.1 \left[ 24.55 \right]\\
                        &= 2.455
    \end{align*}
    \subsubsection{Timestep 2}
    \begin{align*}
        q_{\pi}(X, U1) &= q_{\pi}(X, U1) + \alpha \left[ R_{t+1} + \gamma R_{t+2} + \gamma^2 q_{\pi}(S_{t+2}, A_{t+2}) - q_{\pi}(X, U1) \right]\\
                      &= 2.2 + 0.1 \left[ 24 + 0.5 \cdot 16 + 0.5^2 \cdot q_{\pi}(T) - 2.2 \right]\\
                      &= 2.2 + 0.1 \left[ 24 + 8 + 0 - 2.2 \right]\\
                      &= 2.2 + 0.1 \left[ 29.8 \right]\\
                        &= 2.2 + 2.98\\
                        &= 5.18
    \end{align*}
    \subsubsection{Timestep 3}

    \begin{align*}
        q_{\pi}(X, U1) &= q_{\pi}(X, U1) + \alpha \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) - q_{\pi}(X, U1) \right]\\
                        &= 5.18 + 0.1 \left[ 16 + 0.5 \cdot q_{\pi}(T) - 5.18 \right]\\
                        &= 5.18 + 0.1 \left[ 16 + 0 - 5.18 \right]\\
                        &= 5.18 + 0.1 \left[ 10.82 \right]\\
                        &= 6.262
    \end{align*}


\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 2]
    What is the purpose of introducing Control Variates in per-decision importance sampling?

    \subsection{Answer}

    The purpose of introducing Control Variates in per-decision importance sampling is to reduce the 
    variance of $G$. This is done by using a linear combination of the original estimate and a
    control variate term. The updated equation is shown below:

    \[
        G_{t:h} = \rho_t (R_{t+1} + \gamma G_{t+1:h}) + (1-\rho_t) V_{h-1} (S_t)
    \]

    Where the second term is the control variate term.

\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\nobreak\extramarks{Problem 3}{}\nobreak{}

\begin{homeworkProblem}[Problem 3]
    In off-policy learning, what are the pros and cons of the Tree-Backup algorithm versus off-policy
    SARSA (comment on the complexity, exploration, variance, and bias, and others)?

    \subsection{Answer}

    The pros and cons of the Tree-Backup algorithm versus off-policy SARSA are as follows:

    \begin{itemize}
        \item \textbf{Complexity:} The complexity of the Tree-Backup algorithm is $O(n)$, where $n$ is the number of steps. The complexity of off-policy SARSA is $O(1)$.
        \item \textbf{Exploration:} The Tree-Backup algorithm explores the environment by following the policy $\pi$ and then following the behavior policy $\mu$ for the remaining steps. Off-policy SARSA explores the environment by following the behavior policy $\mu$ for all steps.
        \item \textbf{Variance:} The variance of the Tree-Backup algorithm is lower than that of off-policy SARSA\@. This is because the Tree-Backup algorithm uses a control variate term to reduce the variance of the estimate.
        \item \textbf{Bias:} The bias of the Tree-Backup algorithm is higher than that of off-policy SARSA\@. This is because the Tree-Backup algorithm uses a control variate term to reduce the variance of the estimate.
        \item \textbf{Others:} The Tree-Backup algorithm is an on-policy algorithm, while off-policy SARSA is an off-policy algorithm.
    \end{itemize}
\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 4}{}\nobreak{}

\begin{homeworkProblem}[Problem 4]
    Assume that we have two states $x$ and $y$ with the current value of $V(x) = 10$, $V(y) = 1$. We
    run an episode of $\{x, 3, y, 0, y, 5, T\}$. What's the new estimate of $V(x)$, $V(y)$ using TD (assume
    step size $\alpha = 0.1$ and discount rate $\gamma = 0.9$).

    \subsection{Answer}

    The new estimate of $V(x)$ is as follows:

    \begin{align*}
        V(x) &= V(x) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(x) \right]\\
             &= 10 + 0.1 \left[ 3 + 0.9 \cdot 1 - 10 \right]\\
             &= 10 + 0.1 \left[ 3.9 - 10 \right]\\
             &= 10 + 0.1 \left[ -6.1 \right]\\
             &= 10 - 0.61\\
             &= 9.39
    \end{align*}

    However, $V(y)$ gets updated twice in this episode. The first update is as follows:

    \begin{align*}
        V(y) &= V(y) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(y) \right]\\
             &= 1 + 0.1 \left[ 0 + 0.9 \cdot 1 - 1 \right]\\
                &= 1 + 0.1 \left[ 0.9 - 1 \right]\\
                &= 1 + 0.1 \left[ -0.1 \right]\\
                &= 1 - 0.01\\
                &= 0.99
    \end{align*}

    The second update is as follows:

    \begin{align*}
        V(y) &= V(y) + \alpha \left[ R_{t+1} + \gamma V(S_{t+1}) - V(y) \right]\\
             &= 0.99 + 0.1 \left[ 5 + 0.9 \cdot 0 - 0.99 \right]\\
                &= 0.99 + 0.1 \left[ 5 - 0.99 \right]\\
                &= 0.99 + 0.1 \left[ 4.01 \right]\\
                &= 0.99 + 0.401\\
                &= 1.391
    \end{align*}

    Therefore, the new estimate of $V(x)$ is $9.39$ and the new estimate of $V(y)$ is $1.391$.

\end{homeworkProblem}

\end{document}