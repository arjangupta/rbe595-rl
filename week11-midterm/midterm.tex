\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage[fleqn]{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage{hyperref}

\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}

\DeclareMathOperator{\EX}{\mathbb{E}}% expected value

\graphicspath{{./images/}}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClassShort\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem {#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
    % \stepcounter{#1}
    \nobreak\extramarks{{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}

\newcommand{\problemNumber}{0.0}

\newenvironment{homeworkProblem}[1][-1]{
    \renewcommand{\problemNumber}{{#1}}
    \section{\problemNumber}
    \setcounter{partCounter}{1}
    \enterProblemHeader{\problemNumber}
}{
    \exitProblemHeader{\problemNumber}
}

%
% Homework Details
%   - Title
%   - Class
%   - Author
%

\newcommand{\hmwkTitle}{Midterm Exam}
\newcommand{\hmwkClassShort}{RBE 595}
\newcommand{\hmwkClass}{RBE 595 --- Reinforcement Learning}
\newcommand{\hmwkAuthorName}{\textbf{Arjan Gupta}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass}}\\
    \textmd{\textbf{\hmwkTitle}}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d}}{\mathrm{d}#2} \left(#1\right)}

% For compact derivatives
\newcommand{\derivcomp}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #2} \left(#1\right)}

% For compact partial derivatives
\newcommand{\pderivcomp}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\nobreak\extramarks{Problem 1}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 1]
    Consider two random variables with distributions below:

    \[
        p = \{0.2, 0.2, 0.2, 0.2, 0.1, 0.1\}
    \]
    \[
        q = \{0.75, 0.0625, 0.0625, 0.0625, 0.0625\} 
    \]

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.3]{prob1_histos.png}
    \end{figure}

    A. [4 points] Calculate the entropy for each variable.\\
    B. [4 points] Intuitively how can you tell which variable has a higher entropy without
    calculating the entropy numerically? What does higher entropy mean?

    \subsection{Answer}

    A. The entropy for each variable is given by:

    \begin{align*}
        H(p) &= -\sum_{i=1}^6 p_i \log_2 p_i \\
        &= -\left( 0.2 \log_2 0.2 + 0.2 \log_2 0.2 + 0.2 \log_2 0.2 + 0.2 \log_2 0.2 + 0.1 \log_2 0.1 + 0.1 \log_2 0.1 \right) \\
        &= -(4*0.2 \log_2 0.2 + 2*0.1 \log_2 0.1) \\
        &= 2.5219
    \end{align*}

    {\setlength{\mathindent}{1cm}
    \begin{align*}
        H(q) &= -\sum_{i=1}^5 q_i \log_2 q_i \\
        &= -\left( 0.75 \log_2 0.75 + 4*0.0625 \log_2 0.0625 \right) \\
        &= 1.3112
    \end{align*}}

    B. Entropy is defined as the lack of expected information, or the 
    `surprise'/uncertainty of a random variable. 
    We can tell that $q$ has a higher entropy than $p$ without calculating the entropy,
    because the histogram for $q$ shows that there is a `surprising' value of 0.75, which
    goes against the trend of the other values (which are all 0.0625). On the other hand,
    the histogram for $p$ shows that all the values are fairly close to each other. In general,
    higher entropy means that the random variable has more uncertainty,
    so the likelihood of encountering a value closer to the expected value is lower.



\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 2]
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.5]{prob2_prompt.png}
    \end{figure}

    \subsection{Answer}

    Option F (B and D) is correct.\\

    I have drawn the backup diagram for $v_*$ as given below:

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.5]{vstar_drawio.png}
    \end{figure}

    \textbf{Explanation}

    $v_*$ is the optimal state-value function. The backup diagram for $v_*$ shows us that,
    if we start at a state s, we choose the action that maximizes the value of the state-action
    pair, and then we take the action. The action choice is taken using our current policy,
    $\pi$. Once we take the action, we end up in a new state, $s'$, and we get a reward, $r$.
    The resultant state $s'$ as well as the reward $r$ are chosen according
    to the dynamics of the environment, $p(s', r | s, a)$, which is not something we can control.
    In summary, the optimal state value function chooses the action that maximizes the value
    of the state-action value.\\

\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\nobreak\extramarks{Problem 3}{}\nobreak{}

\begin{homeworkProblem}[Problem 3]
    Consider a vehicle with 4 actions (left, right, up, down). There's no uncertainty in the outcome
    of the action (i.e. when left is commanded, the left state is achieved). The actions that cause
    the vehicle outside the grid, leave the state unchanged.
    The reward for all transition is -1 except when the goal is reached where the reward is zero.
    Discount factor $\gamma = 1$.\\
    The figure on left shows the name of the states and figure on the right shows the state-value
    $V(s)$, for each state under a uniform random policy.

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.5]{prob3_prompt.png}
    \end{figure}

    A. [4 points] What is $q(k, down)$?\\
    B. [4 points] What is $q(g, down)$?\\
    C. [4 points] What is $V(f)$?\\

    \subsection{Answer}

    \textbf{A.} We know the equation for $q(s, a)$ is given by,

    \begin{align*}
        q(s, a) &= \sum_{s', r} p(s', r | s, a) \left[ r + \gamma V(s') \right]
    \end{align*}

    We can calculate $q(k, down)$ as follows:

    \begin{align*}
        q(k, down) &= \sum_{s', r} p(s', r | k, down) \left[ r + \gamma V(s') \right] \\
        &= 1 \left[ 0 + 1 \cdot 0 \right]\\
        &= 0
    \end{align*}

    \textbf{B.} We can calculate $q(g, down)$ as follows:

    \begin{align*}
        q(g, down) &= \sum_{s', r} p(s', r | g, down) \left[ r + \gamma V(s') \right] \\
        &= 1 \left[ -1 + 1 \cdot -14 \right]\\
        &= -15
    \end{align*}

    \textbf{C.} We know the equation for $V(s)$ is given by,

    \begin{align*}
        V(s) &= \sum_{a} \pi(a | s) q(s, a)
    \end{align*}
    
    We can calculate $V(f)$ as follows:

    \begin{align*}
        V(f) &= \sum_{s', r} p(s', r | f, \pi(f)) \left[ r + \gamma V(s') \right] \\
        &= 0.25 \left[ -1 + 1 \cdot -18 \right] + 0.25 \left[ -1 + 1 \cdot -20 \right] + 0.25 \left[ -1 + 1 \cdot -20 \right] + 0.25 \left[ -1 + 1 \cdot -18 \right]\\
        &= 0.25 \left[ -19 \right] + 0.25 \left[ -21 \right] + 0.25 \left[ -21 \right] + 0.25 \left[ -19 \right]\\
        &= 0.25 \left[ -80 \right]\\
        &= -20
    \end{align*}
    
\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 4}{}\nobreak{}

\begin{homeworkProblem}[Problem 4]
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.8]{prob4_prompt.png}
    \end{figure}

    \subsection{Answer}

    \textbf{A.} The target value for SARSA is given by
    $R + \gamma Q(S_{t+1}, A_{t+1})$.\\
    The update rule for SARSA is given by
    $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ target - Q(S_t, A_t) \right]$.\\

    \textit{For left action at $s'$}:\\
    The target SARSA value at $q(s, left)$ is given by $1 + 0.5 \cdot 2 = 2$\\
    The update value for SARSA is given by $1 + 0.1 \cdot (2 - 1) = 1.1$\\

    \textit{For right action at $s'$}:\\
    The target SARSA value at $q(s, right)$ is given by $1 + 0.5 \cdot 4 = 3$\\
    The update value for SARSA is given by $1 + 0.1 \cdot (3 - 1) = 1.2$\\

    \vspace{0.75cm}

    \textbf{B.} The target value for Expected SARSA is given by
    $R + \gamma \sum_a \pi(a | S_{t+1}) Q(S_{t+1}, a)$.\\
    The update rule for Expected SARSA is given by
    $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ target - Q(S_t, A_t) \right]$.\\

    The policy distribution for $s'$ is given by $\pi(left | s') = 0.3$ and $\pi(right | s') = 0.7$.\\
    
    The target Expected SARSA value at $q(s, left)$ is given by
    $1 + 0.5 (0.3 \cdot 2 + 0.7 \cdot 4) = 2.7$\\
    The update value for Expected SARSA is given by $1 + 0.1 \cdot (2.7 - 1) = 1.17$\\

    \vspace{0.5cm}

    \textbf{C.} The target value for Q-learning is given by
    $R + \gamma \max_a Q(S_{t+1}, a)$.\\
    The update rule for Q-learning is given by
    $Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[ target - Q(S_t, A_t) \right]$.\\

    The target Q-learning value at $q(s, left)$ is given by $1 + 0.5 \cdot 4 = 3$\\
    The update value for Q-learning is given by $1 + 0.1 \cdot (3 - 1) = 1.2$\\

    \vspace{0.6cm}

    \textbf{D.} The distribution of the action at $s'$ does not affect the target value for Q-learning,
    because the target value for Q-learning is given by $R + \gamma \max_a Q(S_{t+1}, a)$, which
    does not depend on the action distribution at $s'$.\\

    \vspace{0.5cm}

    \textbf{E.} With the new batch of initial estimates, and applying double Q-learning,\\
    our $Q_1$ target is given by
    $R + \gamma Q_2(S_{t+1}, \arg\max_a Q_1(S_{t+1}, a))$\\
    and our $Q_2$ target is given by
    $R + \gamma Q_1(S_{t+1}, \arg\max_a Q_2(S_{t+1}, a))$\\

    The update rule for $Q_1$ is given by
    $Q_1(S_t, A_t) \leftarrow Q_1(S_t, A_t) + \alpha \left[ targetQ_1 - Q_1(S_t, A_t) \right]$.\\
    The update rule for $Q_2$ is given by
    $Q_2(S_t, A_t) \leftarrow Q_2(S_t, A_t) + \alpha \left[ targetQ_2 - Q_2(S_t, A_t) \right]$.\\

    The target value for $q(s, left)$ is given by $1 + 0.5 \cdot q'(s', right) = 1 + 0.5 \cdot 3 = 2.5$\\
    The update value for $q(s, left)$ is given by $1 + 0.1 \cdot (2.5 - 1) = 1.15$\\
    
    \textbf{F.}
    
    The target value for $q'(s, left)$ is given by $1 + 0.5 \cdot q(s', right) = 1 + 0.5 \cdot 4 = 3$\\
    The update value for $q'(s, left)$ is given by $2 + 0.1 \cdot (3 - 2) = 2.1$\

\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 5}{}\nobreak{}

\begin{homeworkProblem}[Problem 5]
    As we discussed, n-step off-policy return via bootstrapping can be written as:
    \begin{align}
        G_{t:h} = \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right) 
    \end{align}
    where $\rho_t = \frac{\pi(A_t | S_t)}{b(A_t | S_t)}$
    is the importance sampling ratio between target and behavior policy.\\
    Using control variates, the return can be written as:
    \begin{align}
        G_{t:h} = \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right) + (1 - \rho_t) V_{h-1} (S_t)
    \end{align}
    • [8 points] Prove that introducing the control variates in equation (2) does not add any
    bias to the original return (i.e.\ equation (1)) in expectation.

    \subsection{Answer}

    If equation (2) does not add any bias to the original return, then we would be able to state that,
    \begin{align}\label{prob5toprove}
        \EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right) ] = \EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right) + (1 - \rho_t) V_{h-1} (S_t) ]
    \end{align}

    This is what we have to prove. Let us first consider the right hand side of the equation (\ref{prob5toprove}).
    We can expand it as follows:
    \begin{align*}
        &\EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right)] + \EX[(1 - \rho_t) V_{h-1} (S_t) ]\\
        = &\EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right)] + \EX[(V_{h-1} (S_t) - \rho_t V_{h-1} (S_t) ]\\
        = &\EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right)] + \EX[V_{h-1} (S_t)] - \EX[\rho_t V_{h-1} (S_t) ]
    \end{align*}

    However, $\rho_t$ is independent of $V_{h-1} (S_t)$ because the ratio of the target policy
    and the behavior policy is independent of the value function. Therefore, we can further
    write the right hand side of the equation (\ref{prob5toprove}) as,
    \begin{align}\label{prob5rhs}
        \EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right)] + \EX[V_{h-1} (S_t)] - \EX[\rho_t] \EX[V_{h-1} (S_t) ]
    \end{align}

    Furthermore, the expectation of the importance sampling ratio is given by,

    \begin{align*}
        \EX[\rho_t] &= \EX \left[ \frac{\pi(A_t | S_t)}{b(A_t | S_t)} \right]
        = \sum_{a} b(a | S_t) \frac{\pi(a | S_t)}{b(a | S_t)} = \sum_{a} \pi(a | S_t) = 1
    \end{align*}

    Which we can substitute into equation (\ref{prob5rhs}) to get,

    \begin{align*}
        &\EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right)] + \EX[V_{h-1} (S_t)] - \EX[V_{h-1} (S_t) ]\\
        = &\EX [ \rho_t \left( R_{t+1} + \gamma G_{t+1:h} \right)]
    \end{align*}

    Which is the same as the left hand side of the equation (\ref{prob5toprove}). Therefore,
    we have proved that equation (2) does not add any bias to the original return in expectation.

\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 6}{}\nobreak{}

\begin{homeworkProblem}[Problem 6]
    [8 points] If you were to design an algorithm called TD-search by modifying the Monte-Carlo
    Tree Search, how do you go about it. In particular, which part of the MCTS you would modify
    and how?

    \subsection{Answer}

    TD methods use bootstrapping, whereas MCTS does not. Therefore, to design TD-search by
    modifying MCTS, we can incorporate bootstrapping into MCTS. Specifically, we can modify
    the simulation phase of MCTS to use TD methods.\\
    
    By default,
    MCTS uses the rollout algorithm to simulate the environment. In rollout, we simulate
    trajectories by starting from a state, then choosing actions randomly until we reach
    a terminal state. The randomness of the action choice is often a uniform distribution.
    After taking each action, we record the reward observed. Once we reach a terminal state,
    we take the average of the rewards observed in the trajectory, and use that as the
    estimated reward for the state we started from. So, suppose we took 3 actions before
    reaching a terminal state, where we observed rewards of 1, 2, and 3. Then, we would
    take the average of these rewards: $(1 + 2 + 3)/3 = 2$.
    Then we can proceed to backup this
    value in the tree.\\

    However, if we want to use TD methods, we would not choose actions according to
    a uniform random policy like the way traditional rollout works.
    Instead, we would choose actions according to the policy
    that we are trying to learn, i.e. derived from the $Q$-values. However, we can still
    make this policy $\epsilon$-greedy, so that we can still explore in the planning phase.
    Additionally, since
    this is a simulation, there is no harm in using an on-policy method like SARSA (because
    we are not actually taking actions in the real environment, so we do not have to worry
    about `bad' results from the on-policy method).
    So, for the simulation phase of TD-search, we would take the following algorithmic steps:

    \begin{enumerate}
        \item Start at the state $S$ we are simulating from.
        \item Choose the action from our current policy, i.e. the one 
        that maximizes the $Q$-value of the state-action pair, or if the random number
        is less than $\epsilon$, then choose a random action (this is normal $\epsilon$-greedy
        exploration).
        \item Take the action, and observe the reward $R$ and the record next state $S'$. Use
        $R$ to update our current average reward estimate for the state we started from. Say that
        this average reward estimate is $R_{avg}$.
        \item Update the $Q$-value of the state-action pair using SARSA, i.e. given 
        by the following update rule:
        \begin{align*}
            Q(S, A) \leftarrow Q(S, A) + \alpha \left[ R + \gamma Q(S', A') - Q(S, A) \right]
        \end{align*}
        \item Set $S = S'$, set $A = A'$, and repeat steps 2-4 until we reach a terminal state.
        \item Backup the value of the state we started from.
    \end{enumerate}

    The backup step is similar to backup step in traditional MCTS, i.e. we take the $R_{avg}$
    that we calculated in step 3 of the TD simulation, and we backup this value in the tree. This
    way, we can use the benefits of bootstrapping in MCTS.\\

\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 7}{}\nobreak{}

\begin{homeworkProblem}[Problem 7]
    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.5]{prob7_prompt.png}
    \end{figure}

    \subsection{Answer}

    \textbf{A.} We can use the following flowchart to choose the next node:

    \begin{figure}[h!]
        \centering
        \includegraphics[scale=0.37]{prob7_flowchart.png}
    \end{figure}

    The UCB1 formula is given by,

    \begin{align*}
        UCB1(S_i) = \frac{v_i}{n_i} + C \sqrt{\frac{\ln N_i}{n_i}}
    \end{align*}

    Where,\\
    $n_i$ is the number of times we have visited node $S_i$,\\
    $N_i$ is the number of times we have visited the parent of node $S_i$,\\
    $v_i$ is the value of node $S_i$, and\\
    $C$ is a constant that we choose.\\

    Starting at A, we choose B because by the UCB1 algorithm,

    \begin{align*}
        \frac{2}{6} + 0.5 \sqrt{\frac{\ln 8}{6}} &> \frac{0}{2} + 0.5 \sqrt{\frac{\ln 8}{2}}\\
        0.62 &> 0.51
    \end{align*}

    Then, at B, we pick E because,

    \begin{align*}
        \frac{-1}{3} + 0.5 \sqrt{\frac{\ln 6}{3}} &< \frac{0}{2} + 0.5 \sqrt{\frac{\ln 6}{2}}\\
        0.053 &< 0.473
    \end{align*}

    Then, at E, we pick H because that is the only option. It is possible that H is
    the only option because there is no valid down action from E.\\

    Hence, H is the next node we pick.

    \vspace{0.5cm}

    \textbf{B.} At H, we assume that both up and down actions are valid, since
    the prompt of this question does not specify otherwise. Therefore, we add
    both up and down nodes from H. Since the first action is up, we choose it.
    Ideally we would choose the action that maximizes the UCB1 value, but
    since we have no information about the value of the nodes, we choose the
    first action. This can also be randomly selected according to the uniform distribution
    (like a coin flip). This up action creates a new state, which is the `J' node.\\

    \textbf{C.} The `n' and `v' of J are 0 and 0 currently because we have not
    visited J yet, and we have not rolled out from it yet.\\

    \textbf{D.} The simulation from J gives a value of 1. So now we can
    backup the tree and update the `n' and `v' of J and its ancestors. The
    new values are given below:

    J\@: $n = 1$, $v = 1$\\
    H\@: $n = 2$, $v = 2$\\
    E\@: $n = 3$, $v = 1$\\
    B\@: $n = 7$, $v = 3$\\
    A\@: $n = 9$, $v = 0$\\

    \textbf{E.} Now that we have run out of time, we compare the up and down
    actions at A. With up, we reach B, which has an average reward is $3/7 = 0.42$.
    With down, we reach C, which has an average reward of $0/2 = 0$. Therefore we
    choose up because in the greedy tree policy we choose the action that maximizes
    the average reward.\\

\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 8}{}\nobreak{}

\begin{homeworkProblem}[Problem 8]

    In this question we are comparing mean and variance of the estimate reward for on-policy and
    off-policy algorithms. Consider the on-policy scenario below where actions are taken under
    target policy $\pi$:\\

    \begin{tabular}{c | c | c}
        Action & Reward & Probability of Action under policy $\pi$\\
        \hline
        right & +10 & 0.9\\
        left & +20 & 0.1\\
    \end{tabular}\\

    A. [2 points] What is the expected value (mean) of estimated reward?\\
    B. [4 points] What is the variance of the estimate reward?\\
    Hint: use the equation $Var[X] = \EX[X^2] - \EX[X]^2$\\

    Now assume that there is a behavior policy that is taking actions with following distribution:

    \begin{tabular}{c | c | c | c}
        Action & Reward & Probability of Action under policy $\pi$& Probability of Action under policy $b$\\
        \hline
        right & +10 & 0.9 & 0.5\\
        left & +20 & 0.1 & 0.5\\
    \end{tabular}

    C. [3 points] Assume that we don't know the distribution of the target policy and we only
    know the ratio $\frac{\pi}{b}$ (importance sampling ratio).\\
    Numerically calculate the expected reward assuming that the action are taken by the
    behavior policy.\\
    D. [5 points] Calculate the variance of the expected reward?\\
    E. [3 points] What is the intuition behind higher variance in off-policy in this example? Can
    you explain how the importance sampling could increase the variance in off-policy
    learning?

    \subsection{Answer}

    \textbf{A.} The expected value of the estimated reward is given by,

    \begin{align*}
        \EX[X] &= \sum_{x} x p(x)\\
        &= 10 \cdot 0.9 + 20 \cdot 0.1\\
        &= 11
    \end{align*}

    \textbf{B.} The variance of the estimated reward is given by,

    \begin{align*}
        Var[X] &= \EX[X^2] - \EX[X]^2\\
        &= (10^2 \cdot 0.9 + 20^2 \cdot 0.1) - 11^2\\
        &= 9
    \end{align*}

    \textbf{C.} The expected reward under the behavior policy
    is given by,

    \begin{align*}
        \EX[X] &= \sum_{x} x p(x)\\
        &= 10 \cdot 0.5 + 20 \cdot 0.5\\
        &= 15
    \end{align*}

    And the overall importance sampling ratio is given by,

    \begin{align*}
        \prod \frac{\pi}{b} &= \frac{0.9}{0.5} \cdot \frac{0.1}{0.5}\\
        &= 0.36
    \end{align*}

    Therefore, the expected reward is,

    \begin{align*}
        \EX[X] &= 15 \cdot 0.36\\
        &= 5.4
    \end{align*}

    \textbf{D.} The variance of the expected reward is given by,

    \begin{align*}
        Var[X] &= \EX[X^2] - \EX[X]^2\\
        &= (10^2 \cdot 0.5 + 20^2 \cdot 0.5) \cdot 0.36 - 5.4^2\\
        &= 60.84
    \end{align*}

    \textbf{E.} The intuition behind higher variance in off-policy learning is that, the behavior
    policy is not the same as the target policy, so the actions taken by the behavior policy
    are not the same as the actions taken by the target policy. Therefore, the expected reward
    is different for the behavior policy and the target policy.\\

    Importance sampling increases the variance in off-policy learning because there
    could be actions that the behavior policy takes that the target policy does not take.
    This would result in a higher variance in the estimated reward, because if we graphed
    the accumulated reward over time, the graph would be more `jagged' than if we were
    using the target policy. The jaggedness comes from the times when importance sampling
    goes to 0 because of actions not taken by the target policy.\\
    
\end{homeworkProblem}

\end{document}