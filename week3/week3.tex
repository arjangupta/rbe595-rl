\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{gensymb}
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}
\usepackage{listings}
\usepackage{hyperref}

\graphicspath{{./images/}}

\usetikzlibrary{automata,positioning}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem {#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{{#1} (continued)}{{#1} continued on next page\ldots}\nobreak{}
    % \stepcounter{#1}
    \nobreak\extramarks{{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}

\newcommand{\problemNumber}{0.0}

\newenvironment{homeworkProblem}[1][-1]{
    \renewcommand{\problemNumber}{{#1}}
    \section{\problemNumber}
    \setcounter{partCounter}{1}
    \enterProblemHeader{\problemNumber}
}{
    \exitProblemHeader{\problemNumber}
}

%
% Homework Details
%   - Title
%   - Class
%   - Author
%

\newcommand{\hmwkTitle}{Week\ \#3 Assignment}
\newcommand{\hmwkClass}{RBE 595 --- Reinforcement Learning}
\newcommand{\hmwkAuthorName}{\textbf{Arjan Gupta}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass}}\\
    \textmd{\textbf{\hmwkTitle}}\\
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[2]{\frac{\mathrm{d}}{\mathrm{d}#2} \left(#1\right)}

% For compact derivatives
\newcommand{\derivcomp}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #2} \left(#1\right)}

% For compact partial derivatives
\newcommand{\pderivcomp}[2]{\frac{\partial #1}{\partial #2}}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\nobreak\extramarks{Problem 1}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 1]
    Suppose $\gamma = 0.8$ and we get the following sequence of rewards\\
    \[R_1 = -2,\ R_2 = 1,\ R_3 = 3,\ R_4 = 4,\ R_5 = 1.0\]
    Calculate the value of $G_0$ by using the equation 3.8 (work forward) and 3.9 (work backward) and
    show they yield the same results.

    \subsection{Answer}

    \subsubsection{Work Forward}
    From the the book, the \textit{discounted return} (equation 3.8), $G_t$, is defined as,

    \[
    \tag{3.8}
        G_t \doteq R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    \]

    Plugging in the values from this problem, we get,
    \vspace{-0.1cm}
    \begin{align*}
        G_0 &= R_1 + \gamma R_2 + \gamma^2 R_3 + \gamma^3 R_4 + \gamma^4 R_5\\
        &= -2 + 0.8 \cdot 1 + 0.8^2 \cdot 3 + 0.8^3 \cdot 4 + 0.8^4 \cdot 1\\
        &= - 2 + 0.8 + 0.64 \cdot 3 + 0.512 \cdot 4 + 0.4096\\
        &= 3.1776
    \end{align*}

    \subsubsection{Work Backward}
    From the book, the ``recursive'' representation of \textit{discounted return} (equation 3.9), $G_t$, is defined as,

    \[
    \tag{3.9}
        G_t \doteq R_{t+1} + \gamma G_{t+1}
    \]

    Plugging in the values from this problem, we get,
    \vspace{-0.1cm}
    \begin{align*}
        G_0 &= R_1 + \gamma G_1\\
        &= -2 + 0.8 \cdot G_1
    \end{align*}
    \vspace{-0.3cm}
    Where we apply 3.8 to $G_1$,
    \vspace{-0.1cm}
    \begin{align*}
        G_1 &= R_2 + \gamma R_3 + \gamma^2 R_4 + \gamma^3 R_5\\
        &= 1 + 0.8 \cdot 3 + 0.8^2 \cdot 4 + 0.8^3 \cdot 1\\
        &= 6.472
    \end{align*}
    \vspace{-0.3cm}
    Therefore,
    \begin{align*}
        G_0 &= -2 + 0.8 \cdot G_1\\
        &= -2 + 0.8 \cdot 6.472\\
        &= 3.1776
    \end{align*}

    \subsubsection{Conclusion}
    We see that both methods yield the same result, $G_0 = 3.1776$.
\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\begin{homeworkProblem}[Problem 2]
    Explain how a room temperature control system can be modeled as an MDP? What are the
    states, actions, rewards, and transitions.

    \subsection{Answer}

    A room temperature control system can be modeled as an MDP as follows.\\

    \textbf{States:} The states are the different temperatures that the room can be in.\\

    \textbf{Actions:} The actions are the different actions that the system can take to change the
    temperature of the room.\\

    \textbf{Rewards:} The rewards are the different rewards that the system can receive for taking 
    an action.\\

    \textbf{Transitions:} The transitions are the different transitions that the system can make
    from one state to another.\\
\end{homeworkProblem}

\nobreak\extramarks{Problem 2}{}\nobreak{}

\pagebreak

\nobreak\extramarks{Problem 3}{}\nobreak{}

\begin{homeworkProblem}[Problem 3]
    What is the reward hypothesis in RL?

    \subsection{Answer}

    The book states the \textit{reward hypothesis} as follows,
    \begin{quote}
        That all of what we mean by goals and purposes can be well thought of as the maximization
        of the expected value of the cumulative sum of a received scalar signal (called reward).
    \end{quote}

    Here is a simple break-down of what the reward hypothesis means:
    \begin{itemize}
        \item In RL, we talk about goals and purposes, which is to find best way to solve a problem.
        \item Any solution to a complex problem can be broken down into a series of steps, and each step can have
        a value associated with it.
        \item We design this `value' associated with each step as a scalar signal which is received from the environment. This scalar signal is called the \textit{reward}.
        \item Therefore, our ultimate goal is to maximize the expected value of the cumulative sum of these rewards.
    \end{itemize}
\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 4}{}\nobreak{}

\begin{homeworkProblem}[Problem 4]
    We have an agent in maze-like world. We want the agent to find the goal as soon as possible.
    We set the reward for reaching the goal equal to $+1$ With $\gamma = 1$. But we notice that the agent
    does not always reach the goal as soon as possible. How can we fix this?

    \subsection{Answer}

    TODO

\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 5}{}\nobreak{}

\begin{homeworkProblem}[Problem 5]
    What is the difference between policy and action?

    \subsection{Answer}
    TODO
    
\end{homeworkProblem}

\pagebreak

\nobreak\extramarks{Problem 6}{}\nobreak{}

\begin{homeworkProblem}[Problem 6]

    \textbf{(Exercise 3.14)}
    Write prompt

    \subsection{Answer}

    TODO

\end{homeworkProblem}

\end{document}